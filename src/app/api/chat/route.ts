import { NextRequest, NextResponse } from 'next/server';
import { openai, CHAT_MODEL } from '@/lib/openai';
import { checkModeration, CRISIS_RESPONSE } from '@/lib/moderation';
import { retrieve, formatEvidence } from '@/lib/rag';
import { getSystemPrompt } from '@/lib/prompt';
import { saveCaseCard, Message } from '@/lib/logger';

export const runtime = 'nodejs';

const MAX_HISTORY_MESSAGES = 12;
const MAX_OUTPUT_TOKENS = 600;
const MAX_REPORT_TOKENS = 1200;

export interface ChatRequest {
  messages: Message[];
  optIn: boolean;
  mode?: 'chat' | 'profile' | 'generate_report';
}

export interface ChatResponse {
  message: string;
  suggestions: string[];
  isCrisis?: boolean;
  isReport?: boolean;
}

function truncateHistory(messages: Message[], maxMessages: number): Message[] {
  if (messages.length <= maxMessages) return messages;
  return messages.slice(-maxMessages);
}

function parseSuggestions(text: string): { message: string; suggestions: string[] } {
  const regex = /ã€å»ºè®®ã€‘(.+?)$/m;
  const match = text.match(regex);

  if (match) {
    const suggestions = match[1]
      .split('|')
      .map(s => s.trim())
      .filter(s => s.length > 0 && s.length <= 20);
    const message = text.replace(regex, '').trimEnd();
    return { message, suggestions };
  }

  return { message: text, suggestions: [] };
}

const CRISIS_SUGGESTIONS = [
  'æˆ‘ç°åœ¨éœ€è¦æœ‰äººé™ª',
  'å¯ä»¥å‘Šè¯‰æˆ‘æ›´å¤šæ±‚åŠ©æ–¹å¼å—',
  'æˆ‘æƒ³èŠç‚¹åˆ«çš„',
];

// ç”»åƒæ¨¡å¼çš„ system prompt
const PROFILE_SYSTEM_PROMPT = `ä½ æ˜¯ Cyber Guide çš„"ç”»åƒåˆ†æå¸ˆ"æ¨¡å¼ã€‚ä½ çš„ä»»åŠ¡æ˜¯é€šè¿‡è½»æ¾çš„å¯¹è¯äº†è§£ç”¨æˆ·ï¼Œæ¯æ¬¡åªé—®ä¸€ä¸ªé—®é¢˜ã€‚

## ä½ è¦äº†è§£çš„ç»´åº¦ï¼ˆä¸è¦ä¸€æ¬¡å…¨é—®ï¼Œè‡ªç„¶åœ°å±•å¼€ï¼‰

1. **åŸºæœ¬æƒ…å†µ**ï¼šåœ¨è¯»/å·²æ¯•ä¸šï¼Ÿä»€ä¹ˆä¸“ä¸šï¼Ÿå¤§å‡ ï¼Ÿ
2. **å½“å‰çŠ¶æ€**ï¼šæœ€è¿‘åœ¨å¿™ä»€ä¹ˆï¼Ÿå¿ƒæƒ…æ€ä¹ˆæ ·ï¼Ÿ
3. **ä¼˜åŠ¿ä¸å…´è¶£**ï¼šè§‰å¾—è‡ªå·±æ“…é•¿ä»€ä¹ˆï¼Ÿå¯¹ä»€ä¹ˆæ„Ÿå…´è¶£ï¼Ÿ
4. **å›°æ‰°ä¸ç„¦è™‘**ï¼šæœ€è¿‘æœ€çƒ¦çš„äº‹æƒ…æ˜¯ä»€ä¹ˆï¼Ÿ
5. **ç›®æ ‡ä¸æ–¹å‘**ï¼šæœ‰æ²¡æœ‰æƒ³åšçš„äº‹ï¼ŸçŸ­æœŸ/é•¿æœŸçš„æƒ³æ³•ï¼Ÿ
6. **è¡ŒåŠ¨åŠ›**ï¼šæ˜¯æƒ³åˆ°å°±åšçš„ç±»å‹ï¼Œè¿˜æ˜¯æƒ³å¾ˆå¤šä½†ä¸å¤ªåŠ¨ï¼Ÿ
7. **ç¤¾äº¤é£æ ¼**ï¼šå–œæ¬¢ç‹¬å¤„è¿˜æ˜¯å’Œæœ‹å‹ä¸€èµ·ï¼Ÿé‡åˆ°å›°éš¾ä¼šæ‰¾äººèŠå—ï¼Ÿ

## å¯¹è¯é£æ ¼
- æ¯æ¬¡åªé—® 1 ä¸ªé—®é¢˜ï¼Œä¸è¦è¿ç¯è¿½é—®
- è¯­æ°”è½»æ¾ï¼Œåƒæœ‹å‹é—²èŠä¸æ˜¯åšé—®å·
- æ ¹æ®ç”¨æˆ·å›ç­”è‡ªç„¶åœ°è¿½é—®æˆ–è·³åˆ°ä¸‹ä¸€ä¸ªç»´åº¦
- é€‚å½“ç»™ä¸€äº›ç®€çŸ­çš„å›åº”ï¼ˆ"å“ˆå“ˆç¡®å®"ã€"èƒ½ç†è§£"ï¼‰å†é—®ä¸‹ä¸€ä¸ª

## æ ¼å¼
æ¯æ¬¡å›å¤æœ€åä¸€è¡Œé™„å¸¦å»ºè®®ï¼š
ã€å»ºè®®ã€‘å»ºè®®1 | å»ºè®®2 | ç»“æŸç”»åƒï¼Œçœ‹çœ‹åˆ†æ`;

// ç”ŸæˆæŠ¥å‘Šçš„ system prompt
const REPORT_SYSTEM_PROMPT = `ä½ æ˜¯ Cyber Guide çš„"ç”»åƒåˆ†æå¸ˆ"ã€‚æ ¹æ®ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½ç”¨æˆ·ç”»åƒåˆ†ææŠ¥å‘Šã€‚

## æŠ¥å‘Šæ ¼å¼è¦æ±‚

ç”¨ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼ˆç”¨ markdown æ ¼å¼ï¼‰ï¼š

### ğŸ¯ ä½ çš„ç”»åƒ

**ä¸€å¥è¯æ¦‚æ‹¬**ï¼šï¼ˆç”¨ä¸€å¥ç”ŸåŠ¨çš„è¯æè¿°è¿™ä¸ªäººï¼‰

### ğŸ“Š ç»´åº¦åˆ†æ

| ç»´åº¦ | åˆ†æ |
|---|---|
| ğŸ“ å½“å‰é˜¶æ®µ | ï¼ˆåœ¨è¯»/æ¯•ä¸šï¼Œä¸“ä¸šæ–¹å‘ï¼‰ |
| ğŸ’ª æ ¸å¿ƒä¼˜åŠ¿ | ï¼ˆ2-3ä¸ªçªå‡ºç‰¹ç‚¹ï¼‰ |
| ğŸ”¥ å…´è¶£æ–¹å‘ | ï¼ˆå¯¹ä»€ä¹ˆæ„Ÿå…´è¶£ï¼‰ |
| ğŸ˜° ä¸»è¦å›°æ‰° | ï¼ˆå½“å‰é¢ä¸´çš„æŒ‘æˆ˜ï¼‰ |
| ğŸ¯ ç›®æ ‡æ¸…æ™°åº¦ | â­â­â­â˜†â˜†ï¼ˆ1-5æ˜Ÿï¼‰ |
| âš¡ è¡ŒåŠ¨åŠ› | â­â­â­â˜†â˜†ï¼ˆ1-5æ˜Ÿï¼‰ |
| ğŸ¤ ç¤¾äº¤åå¥½ | ï¼ˆå†…å‘/å¤–å‘/çµæ´»å‹ï¼‰ |

### ğŸ’¡ å­¦é•¿å»ºè®®

ï¼ˆé’ˆå¯¹è¿™ä¸ªäººçš„å…·ä½“æƒ…å†µï¼Œç»™ 2-3 æ¡å®é™…å¯è¡Œçš„å»ºè®®ï¼Œæ¯æ¡ 1-2 å¥è¯ï¼‰

### ğŸŒŸ ä¸€å¥é¼“åŠ±

ï¼ˆæ ¹æ®ä»–çš„ç‰¹ç‚¹ï¼Œç»™ä¸€å¥çœŸè¯šçš„ã€ä¸ªæ€§åŒ–çš„é¼“åŠ±ï¼Œä¸è¦é¸¡æ±¤ï¼‰

---
æ³¨æ„ï¼šæŠ¥å‘Šè¦åŸºäºå¯¹è¯ä¸­çš„çœŸå®ä¿¡æ¯ï¼Œæ²¡èŠåˆ°çš„ç»´åº¦å°±å†™"æš‚æœªäº†è§£"ï¼Œä¸è¦ç¼–é€ ã€‚è¯­æ°”æ¸©æš–ä½†ä¸è™šä¼ªã€‚`;

export async function POST(request: NextRequest) {
  try {
    const body = await request.json() as ChatRequest;
    const { messages, optIn, mode = 'chat' } = body;

    if (!messages || !Array.isArray(messages) || messages.length === 0) {
      return NextResponse.json(
        { error: 'messages æ•°ç»„ä¸èƒ½ä¸ºç©º' },
        { status: 400 }
      );
    }

    const lastUserMessage = [...messages]
      .reverse()
      .find(m => m.role === 'user');

    if (!lastUserMessage) {
      return NextResponse.json(
        { error: 'æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯' },
        { status: 400 }
      );
    }

    // å®‰å…¨æ£€æŸ¥ï¼ˆæ‰€æœ‰æ¨¡å¼éƒ½éœ€è¦ï¼‰
    const moderationResult = checkModeration(lastUserMessage.content);

    if (moderationResult.isCrisis) {
      console.log('[CRISIS DETECTED]', {
        crisisKeywordsFound: moderationResult.crisisKeywordsFound,
      });

      if (optIn) {
        await saveCaseCard([
          ...messages,
          { role: 'assistant', content: CRISIS_RESPONSE }
        ]);
      }

      return NextResponse.json({
        message: CRISIS_RESPONSE,
        suggestions: CRISIS_SUGGESTIONS,
        isCrisis: true,
      } as ChatResponse);
    }

    // ===== ç”ŸæˆæŠ¥å‘Šæ¨¡å¼ =====
    if (mode === 'generate_report') {
      const completion = await openai.chat.completions.create({
        model: CHAT_MODEL,
        messages: [
          { role: 'system', content: REPORT_SYSTEM_PROMPT },
          ...messages.map(m => ({
            role: m.role as 'user' | 'assistant',
            content: m.content,
          })),
          { role: 'user', content: 'è¯·æ ¹æ®æˆ‘ä»¬åˆšæ‰çš„å¯¹è¯ï¼Œç”Ÿæˆæˆ‘çš„ç”»åƒåˆ†ææŠ¥å‘Šã€‚' },
        ],
        temperature: 0.6,
        max_tokens: MAX_REPORT_TOKENS,
      });

      const report = completion.choices[0]?.message?.content?.trim()
        || 'æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚';

      return NextResponse.json({
        message: report,
        suggestions: [],
        isCrisis: false,
        isReport: true,
      } as ChatResponse);
    }

    // ===== ç”»åƒå¯¹è¯æ¨¡å¼ =====
    if (mode === 'profile') {
      const truncatedMessages = truncateHistory(messages, MAX_HISTORY_MESSAGES);

      const completion = await openai.chat.completions.create({
        model: CHAT_MODEL,
        messages: [
          { role: 'system', content: PROFILE_SYSTEM_PROMPT },
          ...truncatedMessages.map(m => ({
            role: m.role as 'user' | 'assistant',
            content: m.content,
          })),
        ],
        temperature: 0.7,
        max_tokens: MAX_OUTPUT_TOKENS,
      });

      const rawMessage = completion.choices[0]?.message?.content?.trim()
        || 'æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›å¤ã€‚';

      const { message: assistantMessage, suggestions } = parseSuggestions(rawMessage);

      return NextResponse.json({
        message: assistantMessage,
        suggestions,
        isCrisis: false,
      } as ChatResponse);
    }

    // ===== æ™®é€šèŠå¤©æ¨¡å¼ =====
    const retrievalResults = retrieve(lastUserMessage.content, 3);
    const evidence = formatEvidence(retrievalResults);
    const systemPrompt = getSystemPrompt() + evidence;
    const truncatedMessages = truncateHistory(messages, MAX_HISTORY_MESSAGES);

    const completion = await openai.chat.completions.create({
      model: CHAT_MODEL,
      messages: [
        { role: 'system', content: systemPrompt },
        ...truncatedMessages.map(m => ({
          role: m.role as 'user' | 'assistant',
          content: m.content,
        })),
      ],
      temperature: 0.7,
      max_tokens: MAX_OUTPUT_TOKENS,
    });

    const rawMessage = completion.choices[0]?.message?.content?.trim()
      || 'æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›å¤ã€‚è¯·ç¨åå†è¯•ã€‚';

    const { message: assistantMessage, suggestions } = parseSuggestions(rawMessage);

    if (optIn) {
      await saveCaseCard([
        ...messages,
        { role: 'assistant', content: assistantMessage }
      ]);
    }

    return NextResponse.json({
      message: assistantMessage,
      suggestions,
      isCrisis: false,
    } as ChatResponse);

  } catch (error) {
    console.error('[CHAT API ERROR]', error);
    return NextResponse.json(
      { error: 'æœåŠ¡å™¨é”™è¯¯ï¼Œè¯·ç¨åå†è¯•' },
      { status: 500 }
    );
  }
}
